# ============================================================
# AiTril Environment Configuration Example
# ============================================================
# Copy this file to .env and fill in your API keys and configuration
#
# Quick Start:
#   1. Copy: cp .env.example .env
#   2. Add your API keys below
#   3. Run: aitril init (for CLI setup)
#   4. Or: docker-compose up -d (for web interface)
#   5. Access web UI: http://localhost:37142
#
# Version: 0.0.36 (Artifact-based coordination + Verification + Deployment)
# ============================================================


# ============================================================
# AITRIL APPLICATION SETTINGS
# ============================================================

# Output Directory for Local Deployments
# - Where generated files/projects are saved when using "Local File System" deployment
# - Path on HOST system (auto-mounted to container when using Docker)
# - Default: ~/Documents/projects/aitril_outputs
# - Example custom path: /Users/yourname/Projects/aitril_builds
AITRIL_OUTPUTS_DIR=/Users/yourname/Documents/projects/aitril_outputs


# ============================================================
# CLOUD PROVIDER API KEYS (Required for cloud models)
# ============================================================

# OpenAI (GPT models)
# - Get your API key: https://platform.openai.com/api-keys
# - Models: gpt-5.1, gpt-4o, gpt-4-turbo, gpt-4, gpt-3.5-turbo
# - Specialized provider: OpenAICodexProvider (code generation optimized)
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-5.1

# Optional: Override model for specialized code provider
# OPENAI_CODEX_MODEL=gpt-5.1

# Anthropic (Claude models)
# - Get your API key: https://console.anthropic.com/settings/keys
# - Models: claude-opus-4-5-20251101, claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001
# - Specialized provider: ClaudeCodeProvider (agentic code building)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_MODEL=claude-opus-4-5-20251101

# Optional: Override model for specialized code provider
# CLAUDE_CODE_MODEL=claude-opus-4-5-20251101

# Google (Gemini models)
# - Get your API key: https://aistudio.google.com/app/apikey
# - Models: gemini-3-pro-preview, gemini-2.0-flash-exp, gemini-2.0-flash, gemini-1.5-pro
# - Specialized provider: GeminiADKProvider (Android/Kotlin optimized)
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-3-pro-preview

# Optional: Override model for specialized ADK provider
# GEMINI_ADK_MODEL=gemini-3-pro-preview


# ============================================================
# LOCAL MODEL SERVERS (Optional - for privacy/offline use)
# ============================================================

# Ollama (Local LLM server)
# - Install: https://ollama.ai/download
# - Start: ollama serve
# - Pull model: ollama pull llama2
# - When running in Docker, use service name: http://ollama:11434
# - When running locally, use: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Llama.cpp Server (Local LLM server)
# - Build from: https://github.com/ggerganov/llama.cpp
# - Start server: ./server -m path/to/model.gguf --host 0.0.0.0 --port 8080
# - When running in Docker, use service name: http://llamacpp:8080
# - When running locally, use: http://localhost:8080
LLAMACPP_BASE_URL=http://localhost:8080
LLAMACPP_MODEL=default


# ============================================================
# CUSTOM PROVIDERS (Up to 3 additional providers)
# ============================================================
# Add your own LLM endpoints (OpenAI-compatible APIs, custom servers, etc.)
# Use provider_type to specify implementation: "ollama", "llamacpp", or "openai"

# Custom Provider 1
# Example: Together.ai, Groq, or other OpenAI-compatible API
CUSTOM1_API_KEY=optional-api-key
CUSTOM1_MODEL=your-model-name
CUSTOM1_BASE_URL=http://your-custom-server:port
CUSTOM1_PROVIDER_TYPE=ollama

# Custom Provider 2
CUSTOM2_API_KEY=optional-api-key
CUSTOM2_MODEL=your-model-name
CUSTOM2_BASE_URL=http://your-custom-server:port
CUSTOM2_PROVIDER_TYPE=llamacpp

# Custom Provider 3
CUSTOM3_API_KEY=optional-api-key
CUSTOM3_MODEL=your-model-name
CUSTOM3_BASE_URL=http://your-custom-server:port
CUSTOM3_PROVIDER_TYPE=openai


# ============================================================
# DEPLOYMENT CONFIGURATION (Optional)
# ============================================================

# GitHub Pages Deployment
# - Get token: https://github.com/settings/tokens
# - Scope needed: repo (full control of private repositories)
# GITHUB_TOKEN=ghp_your_github_token_here

# AWS EC2 Deployment
# - Get credentials: https://console.aws.amazon.com/iam/
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# Docker Registry (for Docker Hub deployments)
# - Get credentials: https://hub.docker.com/settings/security
# DOCKER_REGISTRY_PASSWORD=your_docker_hub_token


# ============================================================
# DOCKER-SPECIFIC CONFIGURATION
# ============================================================
# When using docker-compose, local services use container names
# The values below are automatically set in docker-compose.yml

# For docker-compose (uncomment if running standalone Docker):
# OLLAMA_BASE_URL=http://ollama:11434
# LLAMACPP_BASE_URL=http://llamacpp:8080


# ============================================================
# ADVANCED SETTINGS (Optional)
# ============================================================

# Enable Specialized Providers (Auto-detects best provider for task)
# - OpenAICodexProvider: Code generation tasks
# - ClaudeCodeProvider: Agentic code building with tools
# - GeminiADKProvider: Android/Kotlin development
# - Default: true
# USE_SPECIALIZED_PROVIDERS=true

# Logging Level
# - Options: DEBUG, INFO, WARNING, ERROR
# - Default: INFO
# LOG_LEVEL=INFO

# Cache Settings
# - Enable caching of LLM responses for faster repeated queries
# - Default: true
# ENABLE_CACHE=true


# ============================================================
# USAGE EXAMPLES
# ============================================================

# Example 1: Quick Start with Cloud Providers (Recommended)
#   1. Copy this file: cp .env.example .env
#   2. Add your OpenAI, Anthropic, and Google API keys above
#   3. Run: aitril tri "Explain quantum computing"
#   4. Result: Three perspectives from GPT-5.1, Claude Opus, and Gemini 3 Pro

# Example 2: Web Interface with Docker (All-in-One)
#   1. Copy this file: cp .env.example .env
#   2. Add your API keys above
#   3. Run: docker-compose up -d
#   4. Open: http://localhost:37142
#   5. Features: Build mode, Tri-lam mode, Consensus mode, Real-time streaming

# Example 3: Build Mode (Code Generation with Multi-Agent Review)
#   1. Open web UI: http://localhost:37142
#   2. Select "Build" mode
#   3. Enter: "Create a React calculator app"
#   4. Watch: Planning → Implementation → Review → Deployment
#   5. Choose: Local, Docker, GitHub Pages, or EC2 deployment

# Example 4: Use Cloud + Local Models (Hybrid Setup)
#   1. Set cloud API keys above
#   2. Start Ollama: docker run -d -p 11434:11434 ollama/ollama
#   3. Pull model: docker exec -it ollama ollama pull llama2
#   4. Enable in settings: aitril init
#   5. Run: aitril tri "your prompt"
#   6. Result: Responses from GPT, Claude, Gemini, and Llama2

# Example 5: Privacy-First (Local Models Only)
#   1. Leave API keys blank
#   2. Start Ollama and/or Llama.cpp servers
#   3. Configure in settings: aitril init
#   4. Disable cloud providers
#   5. Enable local providers
#   6. Run: aitril tri "your prompt"
#   7. Result: All responses from local models (100% offline)

# Example 6: Custom Deployment to GitHub Pages
#   1. Set GITHUB_TOKEN above
#   2. Run build mode: aitril build "Create a portfolio website"
#   3. Select deployment: GitHub Pages
#   4. Enter repo URL
#   5. Result: Automatic git push to gh-pages branch


# ============================================================
# TROUBLESHOOTING
# ============================================================

# Issue: "No API key found"
# - Solution: Ensure .env file is in the same directory as aitril command
# - Verify: cat .env | grep API_KEY (should show your keys)

# Issue: "Connection refused" for local models
# - Solution: Ensure Ollama/Llama.cpp server is running
# - Check: curl http://localhost:11434/api/tags (for Ollama)
# - Check: curl http://localhost:8080/health (for Llama.cpp)

# Issue: Web interface not accessible
# - Solution: Check if container is running: docker ps | grep aitril-web
# - Check port: docker logs aitril-web | grep "Uvicorn running"
# - Expected: http://0.0.0.0:37142

# Issue: Deployments not saving to correct directory
# - Solution: Check AITRIL_OUTPUTS_DIR path exists
# - Verify: ls -la "$AITRIL_OUTPUTS_DIR"
# - Create if needed: mkdir -p ~/Documents/projects/aitril_outputs

# Issue: Docker can't find .env file
# - Solution: Ensure .env is in same directory as docker-compose.yml
# - Verify: docker-compose config | grep OPENAI_API_KEY
# - Should show: OPENAI_API_KEY=sk-...


# ============================================================
# SECURITY NOTES
# ============================================================

# IMPORTANT: Never commit .env to version control!
# - .env is in .gitignore by default
# - If you accidentally committed secrets, rotate all API keys immediately
# - Use environment variables in production: export OPENAI_API_KEY=...

# Best Practices:
# - Rotate API keys regularly
# - Use separate keys for dev/staging/production
# - Set spending limits on provider dashboards
# - Monitor usage: https://platform.openai.com/usage
# - Keep this file secure: chmod 600 .env


# ============================================================
# MORE INFORMATION
# ============================================================

# Documentation: https://github.com/professai/aitril
# Issues: https://github.com/professai/aitril/issues
# PyPI: https://pypi.org/project/aitril/
# Docker Hub: https://hub.docker.com/r/professai/aitril
# Version: 0.0.36

# New in v0.0.36:
# - Artifact-based coordination (full content transfer between agents)
# - File verification (ensures generated files have actual content)
# - Deployment system (Local, GitHub Pages, Heroku, Vercel, Docker Hub)
# - Configurable outputs directory (AITRIL_OUTPUTS_DIR)
# - Specialized providers (Codex, Claude Code, Gemini ADK)
# - Web interface improvements (real-time deployment UI)
