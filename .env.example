# AiTril Environment Configuration Example
# Copy this file to .env and fill in your API keys and configuration

# ========================================
# CLOUD PROVIDER API KEYS
# ========================================

# OpenAI (GPT models)
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-5.1

# Anthropic (Claude models)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_MODEL=claude-opus-4-20250514

# Google (Gemini models)
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-3-pro-preview

# ========================================
# LOCAL MODEL SERVERS
# ========================================

# Ollama (Local LLM server)
# When running in Docker, use service name: http://ollama:11434
# When running locally, use: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Llama.cpp Server (Local LLM server)
# When running in Docker, use service name: http://llamacpp:8080
# When running locally, use: http://localhost:8080
LLAMACPP_BASE_URL=http://localhost:8080
LLAMACPP_MODEL=default

# ========================================
# CUSTOM PROVIDERS (Up to 3)
# ========================================

# Custom Provider 1
# Use provider_type to specify implementation: "ollama", "llamacpp", or "openai"
CUSTOM1_API_KEY=optional-api-key
CUSTOM1_MODEL=your-model-name
CUSTOM1_BASE_URL=http://your-custom-server:port
CUSTOM1_PROVIDER_TYPE=ollama

# Custom Provider 2
CUSTOM2_API_KEY=optional-api-key
CUSTOM2_MODEL=your-model-name
CUSTOM2_BASE_URL=http://your-custom-server:port
CUSTOM2_PROVIDER_TYPE=llamacpp

# Custom Provider 3
CUSTOM3_API_KEY=optional-api-key
CUSTOM3_MODEL=your-model-name
CUSTOM3_BASE_URL=http://your-custom-server:port
CUSTOM3_PROVIDER_TYPE=openai

# ========================================
# DOCKER-SPECIFIC CONFIGURATION
# ========================================

# When using docker-compose, local services use container names
# Uncomment these lines when running with docker-compose:

# OLLAMA_BASE_URL=http://ollama:11434
# LLAMACPP_BASE_URL=http://llamacpp:8080

# ========================================
# USAGE EXAMPLES
# ========================================

# Example 1: Use all cloud providers (OpenAI + Anthropic + Gemini)
#   - Set all three API keys above
#   - Run: aitril tri "your prompt"

# Example 2: Use cloud + local models (5 providers total)
#   - Set cloud API keys
#   - Start Ollama: docker run -d -p 11434:11434 ollama/ollama
#   - Pull model: docker exec ollama ollama pull llama2
#   - Enable Ollama in settings: aitril init
#   - Run: aitril tri "your prompt"

# Example 3: Use only local models
#   - Start Ollama and/or Llama.cpp servers
#   - Disable cloud providers in settings
#   - Enable local providers
#   - Run: aitril tri "your prompt"

# Example 4: Use docker-compose with all services
#   - Copy this file to .env
#   - Run: docker-compose up -d
#   - Access web UI: http://localhost:8888
#   - Or use CLI: docker-compose exec aitril aitril tri "your prompt"
